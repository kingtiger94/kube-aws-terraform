#cloud-config

coreos:
  etcd2:
    proxy: on
    listen-client-urls: https://0.0.0.0:2379
  fleet:
    public-ip: $private_ipv4
    metadata: env=${CLUSTER_NAME},platform=ec2,provider=aws,role=controller
  update:
    reboot-strategy: etcd-lock
  locksmith:
    group: controller
  units:
    - name: locksmithd.service
      command: reload-or-restart
      drop-ins:
        - name: 30-cloudinit.conf
          content: |
            [Unit]
            Requires=etcd2.service
            After=etcd2.service
            [Service]
            Environment="LOCKSMITHD_ETCD_CERTFILE=/etc/etcd/certs/etcd-member.pem"
            Environment="LOCKSMITHD_ETCD_KEYFILE=/etc/etcd/certs/etcd-member-key.pem"
            Environment="LOCKSMITHD_ETCD_CAFILE=/etc/etcd/certs/etcd-member-ca.pem"
            Environment="LOCKSMITHD_ENDPOINT=https://127.0.0.1:2379"
            Environment="LOCKSMITHD_REBOOT_WINDOW_START=05:30"
            Environment="LOCKSMITHD_REBOOT_WINDOW_LENGTH=3h"
    - name: etcd2.service
      command: reload-or-restart
      drop-ins:
        - name: 60-initial-cluster.conf
          content: |
            [Service]
            EnvironmentFile=/etc/sysconfig/initial-cluster
            EnvironmentFile=/etc/etcd/cert-envs
    - name: fleet.service
      command: reload-or-restart
    - name: format-opt-data.service
      command: start
      content: |
        [Unit]
        Description=Formats opt data drive
        [Service]
        Type=oneshot
        RemainAfterExit=yes
        Environment="LABEL=opt-data"
        Environment="DEV=/dev/xvdc"
        ExecStart=-/bin/bash -c "if ! findfs LABEL=$LABEL > /tmp/label.$LABEL; then  wipefs -a -f $DEV && mkfs.ext4 -F -L $LABEL $DEV && echo wiped; fi"
    - name: opt-data.mount
      command: start
      content: |
        [Unit]
        Description=Mount data to /opt/data
        Requires=format-opt-data.service
        After=format-opt-data.service
        [Mount]
        What=/dev/xvdc
        Where=/opt/data
        Type=ext4

# coreos.units.* components
    - name: format-disk.service
      command: start
      content: |
        [Unit]
        Description=Formats the disk drive
        [Service]
        Type=oneshot
        RemainAfterExit=yes
        Environment="LABEL=var-lib-docker"
        Environment="DEV=/dev/xvdb"
        # Do not wipe the disk if it's already being used, so the docker images persistent cross reboot.
        ExecStart=-/bin/bash -c "if ! findfs LABEL=$LABEL > /tmp/label.$LABEL; then wipefs -a -f $DEV && mkfs.ext4 -T news -F -L $LABEL $DEV && echo wiped; fi"
    - name: var-lib-docker.mount
      command: start
      content: |
        [Unit]
        Description=Mount disk to /var/lib/docker
        Requires=format-disk.service
        After=format-disk.service
        Before=docker.service
        [Mount]
        What=/dev/xvdb
        Where=/var/lib/docker
        Type=ext4
    - name: docker.service
      command: start
      drop-ins:
        - name: 60-docker-wait-for-var-lib.conf
          content: |
              [Unit]
              Requires=var-lib-docker.mount
              After=var-lib-docker.mount
              [Service]
              Restart=always
              RestartSec=5
    - name: s3sync.service
      command: start
      content: |
        [Unit]
        Description=Sync files from S3
        Wants=docker.service
        After=docker.service
        ConditionPathExists=/opt/bin/s3sync.json
        [Service]
        EnvironmentFile=/etc/environment
        TimeoutStartSec=10min
        ExecStartPre=-/usr/bin/docker rm s3sync
        ExecStart=/opt/bin/s3sync.sh
        [Install]
        WantedBy=multi-user.target
    - name: install-kubernetes.service
      command: start
      content: |
        [Unit]
        Description=Install Kubernetes binaries
        Requires=docker.service opt-data.mount etcd2
        After=docker.service opt-data.mount etcd2
        [Service]
        Type=oneshot
        RemainAfterExit=true
        ExecStart=/bin/bash -c "docker run --env VERSION="${KUBE_VERSION}" --rm -v /opt/bin:/shared xueshanf/install-kubernetes"
        [Install]
        WantedBy=multi-user.target
    - name: kube-controller-manager.service
      command: start
      content: |
        [Unit]
        Description=Kubernetes Controller Manager
        Documentation=https://github.com/GoogleCloudPlatform/kubernetes
        Requires=install-kubernetes.service
        After=install-kubernetes.service
        [Service]
        ExecStart=/opt/bin/kube-controller-manager \
          --allocate-node-cidrs=true \
          --cloud-provider=aws \
          --cluster-cidr=${KUBE_CLUSTER_CIDR} \
          --cluster-name=${CLUSTER_NAME} \
          --leader-elect=true \
          --master=http://$private_ipv4:8080 \
          --root-ca-file=/var/lib/kubernetes/etcd-member-ca.pem \
          --service-account-private-key-file=/var/lib/kubernetes/etcd-member-key.pem \
          --service-cluster-ip-range=${KUBE_SERVICE_CIDR} \
          --v=2
        Restart=on-failure
        RestartSec=5
        [Install]
        WantedBy=multi-user.target
    - name: kube-scheduler.service
      command: start
      content: |
        [Unit]
        Description=Kubernetes Scheduler
        Documentation=https://github.com/GoogleCloudPlatform/kubernetes
        Requires=install-kubernetes.service
        After=install-kubernetes.service
        [Service]
        ExecStart=/opt/bin/kube-scheduler \
          --leader-elect=true \
          --master=http://$private_ipv4:8080 \
          --v=2
        Restart=on-failure
        RestartSec=5
        [Install]
        WantedBy=multi-user.target
    - name:  kube-apiserver.service
      command: start
      content: |
        [Unit]
        Description=Kubernetes API Server
        Documentation=https://github.com/GoogleCloudPlatform/kubernetes
        Requires=install-kubernetes.service
        After=install-kubernetes.service
        [Service]
        ExecStart=/opt/bin/kube-apiserver \
          --admission-control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota \
          --advertise-address=$private_ipv4 \
          --allow-privileged=true \
          --apiserver-count=3 \
          --authorization-mode=ABAC \
          --authorization-policy-file=/var/lib/kubernetes/policy.jsonl \
          --bind-address=0.0.0.0 \
          --cloud-provider=aws \
          --enable-swagger-ui=true \
          --etcd-servers=https://127.0.0.1:2379 \
          --etcd-cafile=/var/lib/kubernetes/etcd-member-ca.pem \
          --etcd-certfile=/var/lib/kubernetes/etcd-member.pem \
          --etcd-keyfile=/var/lib/kubernetes/etcd-member-key.pem \
          --insecure-bind-address=0.0.0.0 \
          --kubelet-certificate-authority=/var/lib/kubernetes/kube-apiserver-ca.pem \
          --kubelet-client-certificate=/var/lib/kubernetes/kube-apiserver.pem \
          --kubelet-client-key=/var/lib/kubernetes/kube-apiserver-key.pem  \
          --service-account-key-file=/var/lib/kubernetes/kube-apiserver-key.pem \
          --service-cluster-ip-range=${KUBE_SERVICE_CIDR} \
          --service-node-port-range=${KUBE_SERVICE_NODE_PORTS} \
          --tls-cert-file=/var/lib/kubernetes/kube-apiserver.pem \
          --tls-private-key-file=/var/lib/kubernetes/kube-apiserver-key.pem \
          --token-auth-file=/var/lib/kubernetes/token.csv \
          --v=2
        Restart=on-failure
        RestartSec=5
    - name: install-vault.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=Install Vault binary
        Wants=s3sync.service
        After=s3sync.service
        [Service]
        Type=oneshot
        RemainAfterExit=true
        ExecStart=/usr/bin/docker run --rm -v /opt/bin:/tmp vault:${VAULT_RELEASE} cp /bin/vault /tmp/vault
    - name: install-cert.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=Install Kubernetes cert from Vault
        Requires=docker.service
        After=docker.service
        [Service]
        Type=oneshot
        RemainAfterExit=true
        ExecStartPre=/opt/bin/s3sync.sh
        ExecStartPre=/usr/bin/docker run --rm -v /opt/bin:/tmp vault:${VAULT_RELEASE} cp /bin/vault /tmp/vault
        ExecStart=/bin/bash -c "[ -x /opt/bin/vault ] &&  /opt/bin/get-kube-certs.sh  kube-apiserver etcd-member"
write_files:
  - path: /etc/profile.d/alias.sh
    permissions: 0755
    owner: root
    content: |
      role=$(curl 169.254.169.254/latest/meta-data/iam/info -s | \
              jq --raw-output '.InstanceProfileArn' | sed 's%.*instance-profile/%%')
      PS1="\[\033[01;32m\]\u@\h\[\033[01;34m\]-$role \w \$\[\033[00m\] "
  - path: /opt/bin/s3sync.sh
    permissions: 0755
    owner: root
    content: |
        #!/bin/bash
        # Sync files from s3 bucket, based on s3sync.json configuration
        AWS_CONFIG_ENV=/root/.aws/envvars
        S3SYNC_CONF=/opt/bin/s3sync.json
        [[ ! -f $AWS_CONFIG_ENV ]] && echo "$AWS_CONFIG_ENV doesn't exit." && exit 0
        IMAGE=suet/awscli:latest
        if [[ ! -f $S3SYNC_CONF ]];
        then
          echo "$S3SYNC_CONF doesn't exist."
          exit 1
        fi
        arr=( $(jq 'keys[]' $S3SYNC_CONF) )
        for i in $${arr[@]}
        do
          source=$(cat $S3SYNC_CONF | jq -r ".$i.source")
          destination=$(cat $S3SYNC_CONF | jq -r ".$i.destination")
          excludes=$(cat $S3SYNC_CONF | jq -r ".$i.excludes")
          command=$(cat $S3SYNC_CONF | jq -r ".$i.command" )
          if [ "$excludes" = "null" ];
          then
              s3command="aws s3 sync --exact-timestamps $source $destination"
          fi
          # sync s3 apps to destination
          docker rm s3sync &> /dev/null
          docker run --rm --name s3sync -v $destination:$destination --env-file=$AWS_CONFIG_ENV $IMAGE /bin/bash -c "$s3command"
          # Kind of a hack work to fix excuteble permissions
          if [ -d $destination/bin ];
          then
            chmod 755 $destination/bin/*
          fi
          tarballs=$(ls -1 $destination/*.tar.gz $destination/*.tar 2> /dev/null)
          if [ -s "$tarballs" ]; then
            tar zxvf $tarballs -C $destination
            if [ -s "$command" ]; then
              $command
            fi
          fi
        done
  - path: /opt/bin/s3sync.json
    permissions: 0644
    owner: root
    content: |
      {
          "cACerts": {
            "source":   "s3://${AWS_ACCOUNT}-${CLUSTER_NAME}-config/pki",
            "destination": "/opt/etc/vault/ca"
          },
          "artifactsUpload": {
            "source":   "s3://${AWS_ACCOUNT}-${CLUSTER_NAME}-config/artifacts/${MODULE_NAME}/upload",
            "destination": "/root/upload",
            "command": "/root/upload/install.sh"
          },
          "caVaultToken": {
            "source":   "s3://${AWS_ACCOUNT}-${CLUSTER_NAME}-config/pki-tokens",
            "destination": "/opt/etc/pki-tokens"
          }
      }
  - path: /opt/bin/get-kube-certs.sh
    permissions: 0755
    owner: root
    content: |
        #!/bin/bash
        # Request Kubernetes certificates.
        export VAULT_ADDR=https://vault.${CLUSTER_INTERNAL_ZONE}
        export VAULT_CACERT=/opt/etc/vault/ca/ca.pem # cert to communicate with vault server.
        export PATH=/opt/bin/:$PATH
        cert_paths="/var/lib/kubernetes /etc/etcd/certs"
        /opt/bin/s3sync.sh > /dev/null 2>&1

        # Wait for vault service
        retry=5
        until vault status || [[ $retry -eq 0 ]];
        do
          sleep 3
          let "retry--"
        done
        if [ $retry -eq 0 ];
          then
          echo "Vault service is not ready."
          exit 1
        fi

        # Vault PKI Token. We store them in both /etc/etcd/certs and /var/lib/kubernetes directories
        for i in $*
        do
            token_name=$i
            export VAULT_TOKEN=$(cat /opt/etc/pki-tokens/$token_name)
            vault write -format=json \
              ${CLUSTER_NAME}/pki/$token_name/issue/$token_name common_name=$(hostname --fqdn) \
              alt_names="kube-$private_ipv4.cluster.local,kubernetes.default,*.cluster.local,*.${CLUSTER_INTERNAL_ZONE},${KUBE_API_SERVICE},${KUBE_API_DNSNAME}" \
              ttl=43800h0m0s \
              ip_sans="127.0.0.1,$private_ipv4" >  /tmp/ca-bundle.certs
            if [ ! -s /tmp/ca-bundle.certs ]; then
              echo "/tmp/ca-bundle.certs doesn't exist or has zero size."
              exit 1
            fi
            for p in $cert_paths
            do
              mkdir -p $p
              cat /tmp/ca-bundle.certs | jq -r ".data.certificate" > $p/$token_name.pem
              cat /tmp/ca-bundle.certs | jq -r ".data.private_key" > $p/$token_name-key.pem
              cat /tmp/ca-bundle.certs | jq -r ".data.issuing_ca" > $p/$token_name-ca.pem
            done
        done

  - path: /etc/etcd/cert-envs
    permissions: 0644
    owner: root
    content: |
        ETCD_CERT_FILE=/etc/etcd/certs/etcd-member.pem
        ETCD_KEY_FILE=/etc/etcd/certs/etcd-member-key.pem
        ETCD_PEER_CERT_FILE=/etc/etcd/certs/etcd-member.pem
        ETCD_PEER_KEY_FILE=/etc/etcd/certs/etcd-member-key.pem
        ETCD_TRUSTED_CA_FILE=/etc/etcd/certs/etcd-member-ca.pem
        ETCD_PEER_TRUSTED_CA_FILE=/etc/etcd/certs/etcd-member-ca.pem

  - path: /etc/profile.d/locksmithctl.sh
    permissions: 0644
    owner: root
    content: |
      # For locksmothclt client to connect etcd cluster through TLS
      export LOCKSMITHCTL_ETCD_CERTFILE=/etc/etcd/certs/etcd-member.pem
      export LOCKSMITHCTL_ETCD_KEYFILE=/etc/etcd/certs/etcd-member-key.pem
      export LOCKSMITHCTL_ETCD_CAFILE=/etc/etcd/certs/etcd-member-ca.pem
      export LOCKSMITHCTL_ENDPOINT=https://127.0.0.1:2379

  - path: /etc/profile.d/etcdctl.sh
    permissions: 0644
    owner: root
    content: |
      # For etcdctl client to connect server through TLS
      export ETCDCTL_CERT_FILE=/etc/etcd/certs/etcd-member.pem
      export ETCDCTL_KEY_FILE=/etc/etcd/certs/etcd-member-key.pem
      export ETCDCTL_CA_FILE=/etc/etcd/certs/etcd-member-ca.pem
      export ETCDCTL_ENDPOINTS=https://127.0.0.1:2379
  - path: /etc/profile.d/vault.sh
    permissions: 0644
    owner: root
    content: |
      # For vault client to connect server through TLS
      export VAULT_CACERT=/opt/etc/vault/ca/ca.pem
      export VAULT_ADDR=https://vault.${CLUSTER_INTERNAL_ZONE}
      export PATH=$PATH:/opt/bin
  - path: /etc/aws/account.envvars
    permissions: 0644
    owner: root
    content: |
      AWS_ACCOUNT=${AWS_ACCOUNT}
      AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}
      CLUSTER_NAME=${CLUSTER_NAME}
  - path: /root/.aws/envvars
    permissions: 0600
    owner: root
    content: |
      AWS_ACCOUNT=${AWS_ACCOUNT}
      AWS_USER=${AWS_USER}
      AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}
  - path: /root/.aws/config
    permissions: 0600
    owner: root
    content: |
      [default]
      aws_access_key_id=${AWS_ACCESS_KEY_ID}
      aws_secret_access_key=${AWS_SECRET_ACCESS_KEY}
      region=${AWS_DEFAULT_REGION}
